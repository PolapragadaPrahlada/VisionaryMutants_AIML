{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e1d849-2be3-49b6-99e6-3a8637656681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a2e9c20-36ff-4789-b8b6-4f39bf5722a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Global variables to store job data\n",
    "job_data_list = []\n",
    "global_job_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffdb840-3069-4689-9c36-0c2769b78d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(xpath):\n",
    "    try:\n",
    "        element = driver.find_element(By.XPATH, xpath)\n",
    "        text = element.text.strip()\n",
    "        return text\n",
    "    except:\n",
    "        return \"NA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa096582-fe07-4c9a-97e1-2bf05b5923ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(xpath):\n",
    "    try:\n",
    "        element = driver.find_element(By.XPATH, xpath)\n",
    "        html = element.get_attribute('innerHTML')\n",
    "        return html\n",
    "    except:\n",
    "        return \"NA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ae9121-6ef9-442c-921f-79eb8ad21504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_and_reviews(company_text):\n",
    "    reviews = \"NA\"\n",
    "    match = re.search(r'(\\d+\\.\\d+)\\s*Reviews', company_text)\n",
    "    if match:\n",
    "        reviews = match.group(1)\n",
    "        company_text = company_text.replace(match.group(0), \"\").strip()\n",
    "    return company_text, reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5191558d-61b5-40f4-826b-387d04d62788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_key_skills(key_skills_html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(key_skills_html, 'html.parser')\n",
    "        spans = soup.find_all('span')\n",
    "        skills_list = [span.get_text(strip=True) for span in spans]\n",
    "        formatted_skills = ', '.join(skills_list)\n",
    "        return formatted_skills\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning key skills: {e}\")\n",
    "        return \"NA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c6279a-ebdf-4f6a-9614-adf327b21a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_details(job_element):\n",
    "    global global_job_id\n",
    "    try:\n",
    "        job_url = job_element.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "        driver.execute_script(\"window.open(arguments[0], '_blank');\", job_url)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'styles_job-header-container___0wLZ')))\n",
    "\n",
    "        job_title_text = get_text(\"//h1[contains(@class, 'styles_jd-header-title__rZwM1')]\")\n",
    "        company_text_raw = get_text(\"//div[contains(@class, 'styles_jd-header-comp-name__MvqAI')]\")\n",
    "        company_text, reviews_text = extract_company_and_reviews(company_text_raw)\n",
    "        location_text = get_text(\"//div[contains(@class, 'styles_jhc_loc__Du2H')]\")\n",
    "        experience_text = get_text(\"//div[contains(@class, 'styles_jhc_exp_k_giM')]\")\n",
    "        salary_text = get_text(\"//div[contains(@class, 'styles_jhc_salary_jdfEC')]\")\n",
    "\n",
    "        key_skills_html = get_html(\"//div[contains(@class, 'styles_key-skill_GIPn')]\")\n",
    "        key_skills_text = clean_key_skills(key_skills_html)\n",
    "\n",
    "        job_data_list.append({\n",
    "            \"Job ID\": global_job_id,\n",
    "            \"Job Title\": job_title_text,\n",
    "            \"Company\": company_text,\n",
    "            \"Reviews\": reviews_text,\n",
    "            \"Location\": location_text,\n",
    "            \"Experience\": experience_text,\n",
    "            \"Salary\": salary_text,\n",
    "            \"Key Skills\": key_skills_text\n",
    "        })\n",
    "\n",
    "        global_job_id += 1\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing job {global_job_id}: {e}\")\n",
    "        if len(driver.window_handles) > 1:\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c3d9a52-1d75-4cff-b081-897a68735a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs(url, job_count):\n",
    "    page_number = 1\n",
    "    total_jobs_collected = 0\n",
    "\n",
    "    while total_jobs_collected < job_count:\n",
    "        print(f\"Scraping page {page_number}...\")\n",
    "        current_url = url if page_number == 1 else f\"{url.rstrip('-')}-{page_number}\"\n",
    "        driver.get(current_url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, \"srp-jobtuple-wrapper\")))\n",
    "\n",
    "        job_list = driver.find_elements(By.CLASS_NAME, \"srp-jobtuple-wrapper\")\n",
    "        if not job_list:\n",
    "            print(\"No more jobs found or page not loaded correctly.\")\n",
    "            break\n",
    "\n",
    "        for i in range(len(job_list)):\n",
    "            if total_jobs_collected >= job_count:\n",
    "                break\n",
    "            try:\n",
    "                job_element = job_list[i]\n",
    "                extract_job_details(job_element)\n",
    "                total_jobs_collected += 1\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing job element {total_jobs_collected + 1}: {e}\")\n",
    "\n",
    "        if total_jobs_collected < job_count:\n",
    "            page_number += 1\n",
    "            time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993eb54-f244-4e50-9b7e-2a0d1c688f64",
   "metadata": {},
   "source": [
    "# job_title = input(\"Enter the job title (e.g., 'data-scientist'): \").strip().replace(' ', '-')\n",
    "job_count = int(input(\"Enter the number of jobs you want to scrape: \"))\n",
    "job_url = f\"https://www.naukri.com/{job_title}-jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cca4b2-bc37-4e0d-a811-8aeb1ebac593",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_jobs(job_url, job_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d882f-0f5d-484e-bf5e-9f4899b65521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV\n",
    "df = pd.DataFrame(job_data_list)\n",
    "df.to_csv('scraped_jobs.csv', index=False, encoding='utf-8')\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Scraping complete. Data saved to 'scraped_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2310f1e-5a04-4cb8-877d-d1f0e14037d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
